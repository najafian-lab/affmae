# --------------------------------------------------------------------------- #
# Experiment & Logging
# --------------------------------------------------------------------------- #
experiment:
  experiment_name: "AFF_BASE_PRETRAIN_ADJUSTED_LR"
  project: "affmae-pretraining"
  entity: "najafian-lab-2025"
  wandb_enabled: true 
  output_dir: "/homes/iws/ziawang/Documents/lab/affmae_root/output"
  seed: 42
  resume_path: "" 
  
# --------------------------------------------------------------------------- #
# Data 
# --------------------------------------------------------------------------- #
data:
  path: "/var/tmp/ziawang/shards/emdata-{00000..00312}.tar"
  img_size: 512
  in_channels: 1
  batch_size: 52  
  num_workers: 8
  prefetch_factor: 3
  pin_memory: true

# --------------------------------------------------------------------------- #
# Model Architecture
# --------------------------------------------------------------------------- #
model:
  model_type: "aff" # 'aff' or 'vit'
  mask_ratio: 0.5
  patch_size: 8
  
  # aff
  # 65.60M
  aff_embed_dims: [128, 256, 512, 768]
  aff_depths: [3,4,16,2]
  aff_num_heads: [4,8,16,24]
  aff_nbhd_sizes: [64, 64, 64, 64]
  aff_cluster_size: 8
  aff_alpha: 10.0
  aff_global_attention: false
  aff_ds_rates: [0.5, 0.5, 0.5, 0.5]
  aff_mlp_ratio: 3
  aff_merging_method: "cluster"

  # decoder config should always be the between all runs
  # ensure clean and fair comparison
  decoder_embed_dim: 384
  # decoder depth is always 4 since the aff encoder has 4 stages
  decoder_depth: 4
  decoder_num_heads: 6

# --------------------------------------------------------------------------- #
# Training
# --------------------------------------------------------------------------- #
train:
  epochs: 500
  num_accum: 5
  base_lr: 2.5e-4
  weight_decay: 0.05
  beta1: 0.883
  beta2: 0.935
  warmup_steps: 10000
  log_freq: 300
  save_freq: 25
  device: "cuda"